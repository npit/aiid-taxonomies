{
  "name": "Technical Failures",
  "description": "A taxonomy of technical causes of failure in AI system training, design, performance and generalization.",
  "children": [
    {
      "name": "Dataset Issues",
      "description": "Failures related to the data used with the model in different stages (e.g. training, evaluation, deployment, etc.)",
      "children": [
        {
          "name": "Limited Dataset",
          "description": "The case where the size and/or distribution of the training dataset is limited, impacting generalization."
        },
        {
          "name": "Data or Labelling Noise",
          "description": "The system is applied or learns from data that has significant amounts of noise, errors, artifacts and critical outliers in its content and/or annotated labels, which impacts resulting performance and/or generalization."
        },
        {
          "name": "Inadequate Data Augmentation",
          "description": "The system does not apply sufficient data augmentation methods to expand its dataset to a state more representative to real-world data variability."
        },
        {
          "name": "Adversarial Data",
          "description": "The system uses data which includes instances that have unexpected content, labels or characteristics, which have been willingly and maliciously inserted, misconfigured or supplied to the model."
        },
        {
          "name": "Inadequate Data Sampling",
          "description": "The AI system uses data that has been sampled from a larger data pool. That sampling is misconfigured (e.g. in terms of resolution, selection, distribution of instance attributes of interest, biases), resulting in a dataset that may be insufficient for training / inference."
        },
        {
          "name": "Inappropriate Training Content",
          "description": "The learning dataset the system is built with contains significant amounts of instances that are problematic given the target domain (e.g. conveying biases, violent / adult content, etc.)."
        },
        {
          "name": "Dataset Imbalance",
          "description": "An AI system is set to learn from a collection of data that does not provide enough instances for all desired predictions (e.g. classes, range of regression values)."
        },
        {
          "name": "Unauthorized Data",
          "description": "The system is built and/or uses data without proper authorization, consent or licensing."
        },
        {
            "name": "Inadequate Anonymization",
            "description": "The system uses data without proper anonymization / pseudonymization, resulting in leaks of sensitive information."
        },
        {
          "name": "Problematic Input",
          "description": "Observed failures derive from the system being supplied with an incomplete / erroneous inputs, leading to unexpected / erroneous outputs.",
            "children": [
                {
                    "name": "Problematic Features",
                    "description": "The system uses a feature set that is misleading, uncorrelated to the outputs or otherwise problematic, in order to complete its predictive task(s)."
                },
                {
                    "name": "Limited Receptive Field",
                    "description": "The size of the input to the model is limited, leading to the model lacking important information and thus generating predictions in a myopic manner."
                },
                {
                    "name": "Outdated Input",
                    "description": "The system makes a prediction on data that are out of date and do not correspond with the current status of the real-world object they are meant to represent."
                }

            ]
        }
      ]
    },
    {
      "name": "Generalization Failure",
      "description": "A trained AI system fails to perform well once deployed in the real world, where conditions (e.g. statistical properties of input data due to noise, environment, etc.) are novel and considerably different from the training environment.",
      "children": [
        {
          "name": "Concept Drift",
          "description": "The system does not account for concepts of interest changing over time, reflected in statistical properties of input data and their relationship to predicted quantities. "
        },
        {
          "name": "Covariate Shift",
          "description": "The scenario where input data distribution changes between different model application scenarios (e.g. between training and deployment)"
        },
        {
          "name": "Overfitting",
          "description": "The system has adopted an overly complicated solution to fit the training data, capturing noise, outliers and irrelevant input artifacts in its modeling.",
          "children": [
            {
              "name": "Data Memorization",
              "description": "The system overfits by memorizing training data, rather that generalizing to real world cases."
            },
            {
              "name": "Input Sensitivity",
              "description": "The system is sensitive to small changes in its input, thereby generating large changes in its output."
            },
          {
            "name": "Domain Adaptation Deficit",
            "description": "The model is unable to adapt its general knowledge to a specific domain."
          }

          ]
        },
        {
          "name": "Underfitting",
          "description": "The learning model uses weak assumptions, under which it cannot effectively capture the complexity present in the training data, resulting in the generation of inaccurate predictions."
        },
        {
          "name": "Distributional Bias",
          "description": "A trained model reflects biases that exist in its training data that are not representative to the real world, convey meaning that is context-specific and not generally applicable, or perpetuate harful stereotypes."
        },
        {
          "name": "Distributional Artifacts",
          "description": "The model provides irrelevant outputs due to its distributional training, appearing to 'hallucinate' facts that were not provided in their input and cannot be reasonably inferred."
        },
        {
          "name": "Context Misidentification",
          "description": "Cases where the learner fails to detect the correct context of its input data, and produces erroneous predictions as a result."
        }
      ]
    },
    {
      "name": "Tuning Issues",
      "description": "Failures arise due to model configuration, tuning and case-specific details, rather than core capability / methodology-related limitations.",
      "children": [
        {
          "name": "Misconfigured Aggregation",
          "description": "The model's decision arises by considering multiple marginal predictions, the combination of which is skewed (e.g. under/overestimating contribution of a particular component) or otherwise misconfigured.",
          "children": [
            {
            "name": "Overpersonalization",
            "description": "Cases in recommendation / search systems where results are dominated by the personalizattion component, rather than query contents."
            }
            ]

        },
        {
          "name": "Misconfigured Threshold",
          "description": "The model's decision threshold is configured to favor a subset of classes, leading in large performance differences in, e.g. precision and recall."
        },
        {
          "name": "Inadequate Verification",
          "description": "The AI system does not perform adequate verification / quality control on information it presents to the user, resulting in erroneous outputs and/or misinformation."
        },
        {
        "name": "Faulty or Indadequate Preprocessing",
        "description": "Cases where failure could be prevented if additional or better data preprocessing was applied to the system's input data."
        },
        {
        "name": "Inadequate Output Filtering",
        "description": "The model's output lacks adequate filtering, post-processing or cleaning."
        }
      ]
    },
    {
      "name": "Multiagent Issues",
      "description": "Failures emanating from having multiple agents working on similar tasks and/or interacting with each other.",
      "children":[
        {
          "name": "Multiagent Goal Divergence",
          "description": "Cases where failure stems from the goals of multiple AI agents clashing / interfering with each other."
        },
        {
          "name": "Miscoordination",
          "description": "Cases where failure arises from faulty coordination of multiple AI systems acting on shared data."
        }

      ]

    },
    {
      "name": "Underspecification",
      "description": "The system lacks fundamental and/or necessary components / functionalities to safely and effectively deal with the real-world task it is assigned to, as a result of bad design and system specification.",
      "children": [
        {
          "name": "Algorithmic Bias",
          "description": "The AI system implements an algorithm that encapsulates societal biases in its internal logic."
        },
        {
          "name": "Lack of Capability Control",
          "description": "The system is allowed to act in the real world with a impact / capability levels far beyond what its task reasonably requires."
        },
        {
          "name": "Misaligned Objective",
          "description": "Failure outcomes are heavily correlated to crucial task / domain requirements being ignored by the training objective of the deployed model, with a bad proxy being used instead."
        },
        {
          "name": "Unsafe Exposure or Access",
          "description": "The AI system is usable to its intended user base without adequate exposure / access limitations, input / result filtering and general safeguarding measures; as a result, it may produce outputs harmful to end users."
        },
        {
            "name": "Misinformation Generation Hazard",
            "description": "The system has the potential to produce outputs that may result in spreading dangerous misinformation if used maliciously, with no countermeasures for mitigating the latter."
        },
        {
          "name": "Incomplete Data Attribute Capture",
          "description": "Information extraction from real-world instances omit attributes that are important; the resulting features used are incomplete towards faithfully and holistically capture the instance and solve the learning problem effectively."
        },
        {
          "name": "Faulty Interface or Instructions",
          "description": "The usage means of the system (e.g. user interface, physical controls etc.) or provided instructions are problematic, leading to failures."
        },
        {
          "name": "Limited User Access",
          "description": "The system restricts user access / tampering excessively and/or in a way that leads to failure."
        },
        {
          "name": "Untested Deployment",
          "description": "The AI system is deployed in the real world without prior testing to a similar enough setting."
        }
      ]
    },
    {
      "name": "Black Box",
      "description": "Cases where causes of failure include a lack of visibility, transparency or inspectability of how the AI system really operates under the hood, making its outputs unpredictable and difficult to understand.",
      "children": [
        {
          "name": "Lack of Transparency",
          "description": "The system converts input data into outputs without clearly communicating internal computational stages, progress and procedures to outside users."
        },
        {
          "name": "Lack of Explainability",
          "description": "The system converts input data into outputs without its internal workings being explainable to users or even experts. "
        },
        {
          "name": "Lack of Interruptability",
          "description": "The system functions in a way that there is considerable difficulty in easily and safely interrupting its operation once it has started."
        },
        {
          "name": "Lack of Corrigibility",
          "description": "There is considerable difficulty in easily and safely applying corrections to goals and methods used by the system."
        }

      ]
    },
    {
      "name": "Robustness Failure",
      "description": "System failure arises due to the model being sensitive to handling specific scenarios (e.g. different, unexpected, edge cases) of its domain of operation.",
      "children": [
        {
          "name": "Lack of Authenticity Assurance",
          "description": "Failure emerges due to the inability to establish the authenticity of an object related to the AI system."
        },
        {
          "name": "Inadequate Regularization",
          "description": "The AI system pursues its main objective without mitigating factors to prevent excessive optimization and over-fitting."
        },
        {
          "name": "Latency Issues",
          "description": "Failure arises because the system is to slow to respond or act, rather than the content of the response or action."
        },
        {
          "name": "Prompt Injection",
          "description": "A case of gaming vulnerability to prompt-based language models, where the default model instructions can be overriden by carefully crafted prompts in the user's input."
        },
        {
          "name": "Gaming Vulnerability",
          "description": "The system usually operates as expected, but malicious external agents can game its function towards producing undesired results."
        },
        {
          "name": "Reward Hacking",
          "description": "The system games its own reward mechanism by exhibiting undesirable behavior that happens to gain large rewards in ways not expected by the system's designers."
        },
        {
          "name": "Inadequate Sequential Memory",
          "description": "The model does not adequately retain a representation of recent history for memoryful operation over multiple steps, leading to failures over dealing with long data sequences."
        },
        {
          "name": "Lack of Adversarial Robustness",
          "description": "The scenario where a model is sensitive to small changes to input data causing large, undesirable changes in its output, resulting in reduced smoothness and stability."
        },
        {
          "name": "Trojan",
          "description": "The system has been compromised via extenral injection of hidden functionality / data."
        },
        {
          "name": "Backup Failure",
          "description": "The system cannot recover from failure due to subsequent failings in the deployment of backups, countermeasures and fallbacks."
        },
        {
          "name": "Inadequate Fault Tolerance",
          "description": "The system fails to recover from errors / exceptions in a safe and controlled manner, devolving to undefined behavior instead (e.g. crashes, nonsensical / junk outputs, etc.)."
        },
        {
          "name": "Black Swan Event",
          "description": "The AI system fails because highly improbable events / circumstances occur, which the system is not equipped to handle."
        },
        {
          "name": "Inadequate Provenance",
          "description": "The system lacks adequate record-keeping, logging and metadata tracking, making processes like correction, verification and diagnosis difficult and failure-prone."
        },
        {
          "name": "Security Vulnerability",
          "description": "The system lacks adequate measures to protect the security or integrity of the information and data of its end users."
        },
        {
          "name": "Lack of Safety Protocols",
          "description": "System deployment, usage or interaction lacks critical checks, measures and barriers that prevent predictable cases of harm."
        },
        {
          "name": "Hardcoding",
          "description": "Failure arises significantly due to some hard-coded (as opposed to data-driven) information, which is used in the AI model leading to harmful outputs."
        }
      ]
    },
    {
      "name": "Human Error",
      "description": "Cases where failure arises mostly due to mistakes or actions done by humans.",
      "children": [
        {
          "name": "Software Bug",
          "description": "Cases where conventional errors in the system's programming (e.g. logic errors, unhandled exceptions, etc.) contribute to the emergence of the incident."
        },
        {
          "name": "System Manipulation",
          "description": "Cases where deliberate manipulation of elements affecting operation of the AI system lead to the observed harm."
        },
        {
          "name": "Misuse",
          "description": "The system is used with limited regard to official instructions, guidelines, safety measures, or societal impact of its outputs.",
          "children":[

              {
                "name": "Deployment Misconfiguration",
                "description": "General cases where simple corrections to the deployment configuration could prevent the observed failure."
              },
              {
                "name": "Task Mismatch",
                "description": "Cases where the AI system fails at a task which is not reasonably expected to be able to solve, given its design and specifications."
              },
              {
                "name": "Malicious Marketing",
                "description": "The AI system is clearly falsely advertised to operate and/or produce results different than its actual deployment shows."
              },
              {
                  "name": "Harmful Application",
                  "description": "Cases where an AI system is deployed and used as intended, but within a context or use case that results in adverse effects, negative or unethical consequences for individuals, communities, or society at large."
              },
              {
                  "name": "Privacy Concerns",
                  "description": "Cases where an AI system uses private / personal data legally, yet the method, degree or domain these data are used are perceived as problematic and an intrusion of user privacy."
              }
          ]
        }

      ]
    },
    {
      "name": "Hardware Issues",
      "description": "Failures related to the system's hardware.",
      "children": [
          {
            "name": "Insufficient Compute",
            "description":"The system requires more computational resources than it has access to, which contributes to the incident."
          },
        {
          "name": "Hardware Failure",
          "description": "Scenarios where the material system component failures contribute to the emergence of the incident."
        },
        {
          "name": "Scaling Limitations",
          "description": "The system fails by not being unable to handle input load effectively."
        }


      ]
    }
  ]
}
